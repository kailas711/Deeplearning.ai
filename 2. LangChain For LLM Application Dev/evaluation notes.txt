we apply the `apply_and_parse` method because we want to get a dictionary as output not just a string.

## Manual Evaluation

we just generated a bunch of Q/A pairs and saved us a lot of time but there is a catch, there is no way to see what is happening inside the chain. 

- what is the actual prompt? 
- what us going innto the language model?
- what are the the documents that are retrived?

There can be intermediate results for comple x subchains and we're just seeing the final output which is not helpfull. 

`langchain.debug` is a util for debugging , it displays all the information of return values, prompt , etc.


We get preditions for all the examples and uses LLM for evaluation 


from the `preditions` that we did few steps ago, we'll print out `query`(question in this case) which is generated by LLM, `answer` (real answer) which is also generated by LLM and the `result` (predicted answer from the retrival) from the `QAGenerateChain`.


The preicted grade is the evaludated result which is generated by LLM model


Regex or string mathcing won't work i these senarios cuz the conversation is natual language in conversation text hence LLMs can do better at interpreting it.
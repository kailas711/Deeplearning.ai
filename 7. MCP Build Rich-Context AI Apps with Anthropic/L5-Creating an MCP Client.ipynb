{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfda5ce-4f4a-4582-b4ae-ab309696eedf",
   "metadata": {},
   "source": [
    "# Lesson 5: Creating an MCP Client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211ebe3-496b-4f54-b811-1b778787ae8f",
   "metadata": {},
   "source": [
    "In the previous lesson, you created an MCP research server that exposes 2 tools. In this lesson, you will make the chatbot communicate to the server through an MCP client. This will make the chatbot MCP compatible. You will continue from where you left off in lesson 4, i.e., you are provided again with the `mcp_project` folder that contains the `research_server.py` file. You'll add to it the MCP chatbot file and update the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc42aa-75b1-44b5-a3d4-d56f77b34cd9",
   "metadata": {},
   "source": [
    "<img src=\"images/lesson_progression.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6333f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b> To Access the  <code>mcp_project</code> folder :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>L5</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dcfb4-b58c-4ed5-b63f-d6a1b03642d8",
   "metadata": {},
   "source": [
    "## Back to the Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa381b7c-5203-4795-a726-bd7698deaf62",
   "metadata": {},
   "source": [
    "Here are the main code parts (`process_query`, `chat_loop`) from the chatbot example of lesson 3. Notice that the burden of tool definitions and execution is now shifted onto the MCP server, so the chatbot logic only contains code related to processing the user queries and to keeping the chat loop running until the user types `quit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285af47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_stock_price(symbol):\\n    return 1000'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"def get_stock_price(symbol):\n",
    "    return 1000\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1322d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_stock_price',\n",
    "        'description': 'Get the current stock price for any symbol',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'required': ['symbol'],\n",
    "            'properties': {\n",
    "                'symbol': {'type': 'string', 'description': 'The stock symbol (e.g., AAPL, GOOGL)'},\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "024702af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "from ollama import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "response = client.chat(model='qwen3:1.7b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'what is the stock price of AAPL?',\n",
    "  },\n",
    "], tools=[tool],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3baf2999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function(name='get_stock_price', arguments={'symbol': 'AAPL'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message.tool_calls[0].function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94a91987",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m exec(\u001b[43ms\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "exec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "chat_response = chat(\n",
    "    model='gemma3:1b',\n",
    "    messages=[{'role': 'user', 'content': 'what is the capital city of Russia'}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10330992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='gemma3:1b', created_at='2025-06-17T19:20:33.1463469Z', done=True, done_reason='stop', total_duration=6690740100, load_duration=4557004800, prompt_eval_count=16, prompt_eval_duration=666777300, eval_count=37, eval_duration=1462118200, message=Message(role='assistant', content='The capital city of Russia is **Moscow**. \\n\\nWhile Saint Petersburg is the largest city in Russia, Moscow is the political, economic, and cultural center of the country. ðŸ˜Š', images=None, tool_calls=None))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa75ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key =  chat_response.message.tool_calls\n",
    "key = \"some\"\n",
    "not key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96d3102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city of Russia is **Moscow**. It is the largest city in Russia and the political, economic, and cultural center of the country.\n"
     ]
    }
   ],
   "source": [
    "message = chat_response.message.content\n",
    "message = message[message.index('</think>')+8:]\n",
    "print(message.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e6fe7-a727-402b-bb6e-265bf2a580e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv\n",
    "from ollama import chat\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "def process_query(query):\n",
    "    messages = [{'role':'user', 'content':query}]\n",
    "    response = chat(\n",
    "        model='qwen3:1.7b',\n",
    "        messages=messages,\n",
    "        tools=[tool],\n",
    "    )\n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        if not response.message.tool_calls:\n",
    "            # If there are no tool calls, just print the response and exit\n",
    "            reply = response.message.content\n",
    "            if '</think>' in reply:\n",
    "                reply = reply[reply.index('</think>')+8:]\n",
    "            process_query = False\n",
    "            print(reply.strip())\n",
    "        else:\n",
    "            # Handle tool calls\n",
    "            for tool_call in response.message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = tool_call.function.arguments\n",
    "\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                # Execute the tool\n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                \n",
    "                # Add the tool result to messages\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response.message.content,\n",
    "                    \"tool_calls\": [tool_call]\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": str(result),\n",
    "                })\n",
    "\n",
    "                # Get new response with tool result\n",
    "                response = chat(\n",
    "                    model='qwen3:1.7b',\n",
    "                    messages=messages\n",
    "                )\n",
    "\n",
    "                if not response.message.tool_calls:\n",
    "                    reply = response.message.content\n",
    "                    if '</think>' in reply:\n",
    "                        reply = reply[reply.index('</think>')+8:]\n",
    "                    print(reply.strip())\n",
    "                    process_query = False\n",
    "    print(messages)\n",
    "\n",
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03e80b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Calling tool get_stock_price with args {'symbol': 'AMZ'}\n",
      "\n",
      "Error: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764075a-1ca5-4820-8fdb-6c244a7a8324",
   "metadata": {},
   "source": [
    "## Building your MCP Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b159c4-e06e-4a0c-acb4-028e2046ed73",
   "metadata": {},
   "source": [
    "Now you will take the functions `process_query` and `chat_loop` and wrap them in a `MCP_ChatBot` class. To enable the chatbot to communicate to the server, you will add a method that connects to the server through an MCP client, which follows the structure given in this reference code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2225f-f389-4fe6-85c7-6e96aa5bbb18",
   "metadata": {},
   "source": [
    "### Reference Code\n",
    "``` python\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uv\",  # Executable\n",
    "    args=[\"run example_server.py\"],  # Command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    # Launch the server as a subprocess & returns the read and write streams\n",
    "    # read: the stream that the client will use to read msgs from the server\n",
    "    # write: the stream that client will use to write msgs to the server\n",
    "    async with stdio_client(server_params) as (read, write): \n",
    "        # the client session is used to initiate the connection \n",
    "        # and send requests to server \n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection (1:1 connection with the server)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List available tools\n",
    "            tools = await session.list_tools()\n",
    "\n",
    "            # will call the chat_loop here\n",
    "            # ....\n",
    "            \n",
    "            # Call a tool: this will be in the process_query method\n",
    "            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run())\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb960f6-be76-46de-93be-58a62160f3ba",
   "metadata": {},
   "source": [
    "- `async with stdio_client(server_params) as (read, write): `  defines a context manager to first pass in parameters from our servers and establish a connection as subprocess using the `async`. Once we establishes a server connection we're going to get access to a read and write stream that we can then pass to a heigher level class called the `ClientSession`.\n",
    "\n",
    "- In this Client Session when we pass the read and write stream we get access to an underlying connection that allow us to make use of functionality for listing tools, itilializing connections, and doing quite a bit more with other primitive. \n",
    "\n",
    "- The client's job is to query for available tools and take those tool and pass them to a LLM. If there is a need to invoke the MCP server will invoke it and if a tool needs to be executed we'll let the MCP server know what to do. \n",
    "\n",
    "- Since we're using an async enviromnent we'll be moving past the MCP.run and will be using asyncio.run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380fcdc-1703-40a6-9dd0-600c6ba2bdb1",
   "metadata": {},
   "source": [
    "### Adding MCP Client to the Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b442185-9c23-47a2-8a7e-f92928a45bed",
   "metadata": {},
   "source": [
    "The MCP_ChatBot class consists of the methods:\n",
    "- `process_query`\n",
    "- `chat_loop`\n",
    "- `connect_to_server_and_run`\n",
    "  \n",
    "and has the following attributes:\n",
    "- session (of type ClientSession)\n",
    "- anthropic: Anthropic                           \n",
    "- available_tools\n",
    "\n",
    "In `connect_to_server_and_run`, the client launches the server and requests the list of tools that the server provides (through the client session). The tool definitions are stored in the variable `available_tools` and are passed in to the LLM in `process_query`.\n",
    "\n",
    "<img src=\"images/tools_discovery.png\" width=\"400\">\n",
    "\n",
    "\n",
    "In `process_query`, when the LLM decides it requires a tool to be executed, the client session sends to the server the tool call request. The returned response is passed in to the LLM. \n",
    "\n",
    "<img src=\"images/tool_invocation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96d918-1854-49b5-8930-f1a6962a3ee9",
   "metadata": {},
   "source": [
    "Here're the `mcp_chatbot` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c305bb8-6474-420e-82b4-e6daecb65d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_project/mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_project/mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from ollama import chat\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: ClientSession = None\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        response = chat(\n",
    "            model='qwen3:1.7b',\n",
    "            messages=messages\n",
    "        )\n",
    "        process_query = True\n",
    "        while process_query:\n",
    "            if not response.message.tool_calls:\n",
    "                # If there are no tool calls, just print the response and exit\n",
    "                process_query = False\n",
    "                print(response.message.content)\n",
    "            else:\n",
    "                # Handle tool calls\n",
    "                for tool_call in response.message.tool_calls:\n",
    "                    tool_name = tool_call.function.name\n",
    "                    tool_args = tool_call.function.arguments\n",
    "                    tool_id = tool_call.id\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                    \n",
    "                    # Execute the tool\n",
    "                    #result = execute_tool(tool_name, tool_args): not anymore needed\n",
    "                    # tool invocation through the client session\n",
    "                    result = await self.session.call_tool(tool_name, arguments=tool_args)\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response.message.content,\n",
    "                        \"tool_calls\": [tool_call]\n",
    "                    })\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": str(result),\n",
    "                        \"tool_call_id\": tool_id\n",
    "                    })\n",
    "\n",
    "                    # Get new response with tool result\n",
    "                    response = chat(\n",
    "                        model='qwen3:1.7b',\n",
    "                        messages=messages\n",
    "                    )\n",
    "\n",
    "                    if not response.message.tool_calls:\n",
    "                        process_query = False\n",
    "                        print(response.message.content)\n",
    "\n",
    "    \n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                    \n",
    "                await self.process_query(query)\n",
    "                print(\"\\n\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def connect_to_server_and_run(self):\n",
    "        # Create server parameters for stdio connection\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"uv\",  # Executable\n",
    "            args=[\"run\", \"research_server.py\"],  # Optional command line arguments\n",
    "            env=None,  # Optional environment variables\n",
    "        )\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                # Initialize the connection with underlying stuff\n",
    "                await session.initialize()\n",
    "    \n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                \n",
    "                tools = response.tools\n",
    "                print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "                \n",
    "                self.available_tools = [{\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"input_schema\": tool.inputSchema\n",
    "                } for tool in response.tools]\n",
    "    \n",
    "                await self.chat_loop()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df7af0-162a-4419-a188-12a2c6058ab2",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca4737-0d67-4429-9379-8228b5c9ebf1",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48b00b-d256-47f0-8002-170ea74876c0",
   "metadata": {},
   "source": [
    "- To open the terminal, run the cell below.\n",
    "- Navigate to the `mcp_project` directory:\n",
    "    - `cd L5/mcp_project`\n",
    "- Activate the virtual environment:\n",
    "    - `source .venv/bin/activate`\n",
    "- Install the additional dependencies:\n",
    "    - `uv add anthropic python-dotenv nest_asyncio`\n",
    "- Run the chatbot:\n",
    "    - `uv run mcp_chatbot.py`\n",
    "- To exit the chatbot, type `quit`.\n",
    "- If you run some queries and would like to access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L5` -> `mcp_project`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d76196-52a5-4337-86b9-31878f558f99",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf205d7-aef2-4254-a29a-3a06911163b5",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0517c-8571-4c64-88dd-358fab27ed13",
   "metadata": {},
   "source": [
    "- [Quick Start for Client Developpers](https://modelcontextprotocol.io/quickstart/client)\n",
    "- [Writing MCP client](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py)\n",
    "- [Another mcp chatbot example](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28922a-af5f-475e-a56a-ae5e77f94cf7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b> To Access the  <code>mcp_project</code> folder :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and then 3) on \"L5\".\n",
    "<p> â¬‡ &nbsp; <b>To Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0666a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
